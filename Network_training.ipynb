{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c693ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters of your PC (Uncomment if necessary)\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto(device_count = {'GPU': 1 , 'CPU': 8} )\n",
    "#sess = tf.compat.v1.Session(config=config) \n",
    "#K.set_session(sess)\n",
    "\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train catalog\n",
    "train_dir = 'train'\n",
    "\n",
    "# Validation catalog\n",
    "val_dir = 'val'\n",
    "\n",
    "# Test catalog\n",
    "test_dir = 'test'\n",
    "\n",
    "# Current folder\n",
    "curr_fold = os.getcwd()\n",
    "\n",
    "# Image resolution\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# backend Tensorflow, channels_last\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Number of samples for training\n",
    "# nb_train_samples = 110\n",
    "nb_train_samples = sum(1 for i in glob.iglob(train_dir+ '/*/*.png'))\n",
    "print('Number of samples for training:', nb_train_samples)\n",
    "\n",
    "# Number of samples for checking\n",
    "#nb_validation_samples = 24\n",
    "nb_validation_samples = sum(1 for i in glob.iglob(val_dir+ '/*/*.png'))\n",
    "print('Number of samples for checking:', nb_validation_samples)\n",
    "\n",
    "# Number of samples for testing\n",
    "#nb_test_samples = 24\n",
    "nb_test_samples = sum(1 for i in glob.iglob(test_dir+ '/*/*.png'))\n",
    "print('Number of samples for testing:', nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf520d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the number of training images \n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  channel_shift_range=True,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "# image example (Uncomment if necessary)\n",
    "\n",
    "#image_file_name = train_dir + '/Correct_astigmatism_cut/Correct_astigmatism_cut (1).png'\n",
    "#img = image.load_img(image_file_name, target_size=(img_width, img_height))\n",
    "#plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of changed images (Uncomment if necessary)\n",
    "#x = image.img_to_array(img)\n",
    "#x = x.reshape((1,) + x.shape)\n",
    "#i = 0\n",
    "#for batch in train_datagen.flow(x, batch_size=1):\n",
    "    #examples of chenged image (it necessary)\n",
    "    #plt.figure(i)\n",
    "    #imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    #i += 1\n",
    "    #if i % 4 == 0:\n",
    "        #break\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec77173",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the VGG16 network\n",
    "vgg16_net = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "vgg16_net.trainable = False\n",
    "\n",
    "# VGG16 Network summary (Uncomment if necessary)\n",
    "#vgg16_net.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add VGG16 instead input layer in our network\n",
    "model.add(vgg16_net)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Network summary (Uncomment if necessary)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ddb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = curr_fold + '/input_layers_training_results/best_input.hdf5'\n",
    "best_result_dense = [ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', save_best_only=True,save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our network (except VGG16 layers)\n",
    "# !!! Rewrite old training result !!!\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=20,\n",
    "    callbacks=best_result_dense\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3864c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the best of saved weights inside the network\n",
    "model.load_weights(curr_fold + '/input_layers_training_results/best_input.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ca18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the accuracy during all the epochs\n",
    "plt.plot(history.history['accuracy'], label='The percentage of correct answers on the train dataset')\n",
    "plt.plot(history.history['val_accuracy'], label='The percentage of correct answers on the validation dataset')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('The percentage of correct answers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(nb_test_samples)\n",
    "print(batch_size)\n",
    "scores = model.evaluate(test_generator)\n",
    "print(\"Accuracy on test data: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the VGG16 trainable only\n",
    "vgg16_net.trainable = True\n",
    "trainable = False\n",
    "for layer in vgg16_net.layers:\n",
    "    if layer.name == 'block1_conv1':\n",
    "        trainable = True\n",
    "    layer.trainable = trainable\n",
    "    \n",
    "# Network summary (Uncomment if necessary)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=1e-5), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = curr_fold + '/vgg16_layers_training_results/best_conv.hdf5'\n",
    "results_conv = [ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=True,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VGG16 network\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=results_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c437594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the best of saved weights inside the network\n",
    "model.load_weights(curr_fold + '/vgg16_layers_training_results/best_conv.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy during all the epochs\n",
    "plt.plot(history.history['accuracy'], label='The percentage of correct answers on the train dataset')\n",
    "plt.plot(history.history['val_accuracy'], label='The percentage of correct answers on the validation dataset')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('The percentage of correct answers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "scores = model.evaluate(test_generator)\n",
    "print(\"Accuracy on test data: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13052422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters of our network\n",
    "model_json = model.to_json()\n",
    "json_file = open(curr_fold + \"/final_network_params/astigmatism_binary.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(curr_fold + \"/final_network_params/astigmatism_binary.h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c04721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a neural network working on a single image (Uncomment if necessary)\n",
    "\n",
    "#img_path = curr_fold + '/Images for testing final network/Correct_astigmatism_cut (70).png'\n",
    "#img = image.load_img(img_path, target_size=(150,150))\n",
    "#x = image.img_to_array(img)\n",
    "#x /= 255\n",
    "#x = np.expand_dims(x, axis = 0)\n",
    "#plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_layers = loaded_model.get_layer(index=0)\n",
    "#conv_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of convolutional layers - 1,2,4,5,7,8,9,11,12,13,15,16,17    3, 6, 10, 14, 18\n",
    "#activation_model = Model(inputs=conv_layers.input, outputs=conv_layers.layers[11].output)\n",
    "#activation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation = activation_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.matshow(activation[0, :, :, 18], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_per_row = 10\n",
    "#n_filters = activation.shape[-1]\n",
    "#size = activation.shape[2]\n",
    "#n_cols = n_filters // images_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_grid = np.zeros((n_cols * size, images_per_row * size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in range(n_cols):\n",
    "    #for row in range(images_per_row):\n",
    "        #channel_image = activation[0, :, :, col * images_per_row + row]\n",
    "        #channel_image -= channel_image.mean()\n",
    "        #channel_image /= channel_image.std()\n",
    "        #channel_image *= 64\n",
    "        #channel_image += 128\n",
    "        #channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "        #display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale = 1. / size\n",
    "#plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "#plt.grid(False)\n",
    "#plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
